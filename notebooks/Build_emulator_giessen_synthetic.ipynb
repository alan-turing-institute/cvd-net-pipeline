{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Pipeline, Giessen-style synthetic data. 5 sensitive parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Simulation Data and Conduct PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the input and output data. Here we also import the recorded boolean variables where input parameters lead to a failed output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify which dataset \n",
    "n_samples = 500\n",
    "n_params = 5\n",
    "\n",
    "input_file = pd.read_csv(f\"../Emulation/Input/input_{n_samples}_{n_params}params.csv\")\n",
    "output_file = pd.read_csv(f\"../Emulation/Outputs/Output_{n_samples}_{n_params}params/resampled_all_pressure_traces_rv.csv\")\n",
    "\n",
    "\n",
    "bool_exist = False\n",
    "if bool_exist:\n",
    " boolean_index = pd.read_csv(f\"../Emulation/Outputs/Output_{n_samples}_{n_params}params/bool_indices_{n_samples}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for ind in range(len(output_file)): \n",
    "    t = range(100) # Time adjustment\n",
    "    p_pat = output_file.iloc[ind, :100].values # Pressure transient\n",
    "\n",
    "    # Plot the pressure transient for each realization\n",
    "    ax.plot(t, p_pat, label=f'Realisation {ind}')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Time Index')\n",
    "ax.set_ylabel('Pressure (mmHg)')\n",
    "ax.set_title('Pressure Transients in Right Ventricle')\n",
    "\n",
    "# Add legend to the plot\n",
    "# ax.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the input data to remove failed to converge simulations\n",
    "if bool_exist:\n",
    "    cleaned_input_df = input_file.drop(boolean_index['0'].values)\n",
    "   \n",
    "else: \n",
    "    cleaned_input_df = input_file.copy()\n",
    "    \n",
    "cleaned_input_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute PCA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "df = output_file.copy()\n",
    "\n",
    "# Copy the data and separate the target variable (only pressure traces)\n",
    "X = df.iloc[:,:100].copy() # traces only\n",
    "\n",
    "# Create an instance of StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform it - standardize\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Convert to dataframe\n",
    "component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "X_pca = pd.DataFrame(X_pca, columns=component_names, index=df.index)\n",
    "\n",
    "X_pca.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Histograms\n",
    "X_pca.hist(bins=30, figsize=(15, 13), layout=(5, 2), alpha=0.7, color='orange')\n",
    "plt.suptitle('Histograms of the First 10 Principal Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variance(pca, width=8, dpi=100):\n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    n = pca.n_components_\n",
    "    grid = np.arange(1, n + 1)\n",
    "    # Explained variance\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    axs[0].bar(grid, explained_variance_ratio, log=True)\n",
    "    axs[0].set(\n",
    "        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "\n",
    "    # Cumulative Variance\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "    axs[1].semilogy(grid, cumulative_explained_variance, \"o-\")\n",
    "    axs[1].set(\n",
    "        xlabel=\"Component\", title=\"% Cumulative Variance\", \n",
    "    )\n",
    "    # Set up figure\n",
    "    fig.set(figwidth=8, dpi=100)\n",
    "    fig.tight_layout()\n",
    "    return axs\n",
    "\n",
    "plot_variance(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import  PowerTransformer\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                ('scl', StandardScaler()),\n",
    "                ('pca', PCA(n_components=10)),\n",
    "                ('post',   PowerTransformer())\n",
    "            ])\n",
    "\n",
    "signals_pca = pipeline.fit_transform(X_scaled)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=10, nrows=2, figsize=(70, 15))\n",
    "\n",
    "for i in range(signals_pca.shape[1]):\n",
    "    temp = np.zeros(signals_pca.shape)\n",
    "    temp[:, i] = signals_pca[:, i]\n",
    "    \n",
    "    signals_new = pipeline.inverse_transform(temp)\n",
    "    \n",
    "    ax[1][i].hist(signals_pca[:,i], bins=10)\n",
    "    for signal in signals_new:\n",
    "        ax[0][i].plot(signal)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "\n",
    "explained_variance_ratios = []\n",
    "pca_components = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_scaled):\n",
    "    X_train, _ = X_scaled[train_index], X_scaled[test_index]\n",
    "    \n",
    "    pca = PCA(n_components=3)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "    # Store the PCA components\n",
    "    pca_components.append(pca.components_)\n",
    "    # Store the explained variance ratio of this fold\n",
    "    explained_variance_ratios.append(pca.explained_variance_ratio_)\n",
    "\n",
    "\n",
    "explained_variance_ratios = np.array(explained_variance_ratios)\n",
    "\n",
    "mean_explained_variance_ratio = np.mean(explained_variance_ratios, axis=0)\n",
    "std_explained_variance_ratio = np.std(explained_variance_ratios, axis=0)\n",
    "\n",
    "percentage_error = (std_explained_variance_ratio / mean_explained_variance_ratio) * 100\n",
    "print(f'percentage error: \\n{percentage_error}')\n",
    "print(f'explained variance ratios: \\n{explained_variance_ratios}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save PCA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f'mkdir -p ../Emulation/Outputs/Output_{n_samples}_{n_params}params/PCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f'mkdir -p ../Emulation/Outputs/Output_{n_samples}_{n_params}params/PCA')\n",
    "\n",
    "# Save first 3 Principle Component data\n",
    "PC_list = []\n",
    "for i in list(range(10)):\n",
    "\n",
    " PC = X_pca.iloc[:,i]\n",
    " PC.to_csv(f'../Emulation/Outputs/Output_{n_samples}_{n_params}params/PCA/PC{i+1}.csv', index=False)\n",
    " PC_list.append(PC)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_df = pd.DataFrame(PC_list).T\n",
    "PC_df.columns = [\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \n",
    "                 \"PC6\", \"PC7\", \"PC8\", \"PC9\", \"PC10\" ]\n",
    "output = pd.concat([output_file, PC_df], axis=1)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Linear Emulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select relevant inputs only\n",
    "relevant_columns = []\n",
    "for col in cleaned_input_df.columns:\n",
    "    relevant_columns.append(col)\n",
    "    if col == 'T': break\n",
    "\n",
    "#columns_with_multiple_values = df_x.nunique() > 1\n",
    "#filtered_input = df_x.loc[:, columns_with_multiple_values]\n",
    "\n",
    "# Select only first 5 inputs \n",
    "filtered_input = cleaned_input_df[relevant_columns]\n",
    "filtered_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.emulate_linear(filtered_input, output=output['iT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.iloc[:,101:].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store R2 scores and models\n",
    "linear_r2_scores = {}\n",
    "linear_mse_scores = {}\n",
    "linear_rse_scores = {}\n",
    "fitted_models = {}\n",
    "\n",
    "# List of output keys to process\n",
    "#output_keys = ['CO', 'EF', 'mPAP', 'dPAP', 'sPAP', 'PC1', 'PC2', 'PC3', 'PC4']\n",
    "output_keys = output.iloc[:,101:].columns\n",
    "#output_keys = ['t_max_dpdt', 'a_epad', 'epad', 's_a_epad', 's_epad', 'min_dpdt', 'max_dpdt',\n",
    "#               'A_p', 'P_max', 'esp', 'sys', 'EF',  'Ees/Ea', 'PC1', 'PC2', 'PC3']\n",
    "\n",
    "# Iterate through the output keys\n",
    "for key in output_keys:\n",
    "    model, r2, mse, rse = utils.emulate_linear(input=filtered_input, output=output[key])\n",
    "    linear_r2_scores[key] = r2\n",
    "    linear_mse_scores[key] = mse\n",
    "    linear_rse_scores[key] = rse\n",
    "    fitted_models[key] = model\n",
    "\n",
    "# Convert the dictionaries to a DataFrame\n",
    "results_df = pd.DataFrame({'R2_Score': linear_r2_scores, 'MSE': linear_mse_scores,  'RSE': linear_rse_scores, 'Model': fitted_models})\n",
    "# Now `results_df` will be a DataFrame with column names as indices, R2 scores, and models\n",
    "print(results_df)\n",
    "\n",
    "# Save the DataFrame to a CSV file (models will not be saved in this step)\n",
    "results_df.to_csv(f'../Emulation/Outputs/Output_{n_samples}_{n_params}params/Emulators/linear_models_and_r2_scores_{n_samples}.csv')\n",
    "\n",
    "# To save the DataFrame with models, use pickle\n",
    "results_df.to_pickle(f'../Emulation/Outputs/Output_{n_samples}_{n_params}params/Emulators/linear_models_and_r2_scores_{n_samples}.csv')\n",
    "results_df.to_pickle(f'/Users/pmzff/Documents/GitHub/GiessenDataAnalysis/Emulators/linear_models_and_r2_scores_{n_samples}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['Model']['iT'].intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibrate Emulator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrate_output_keys = ['t_max_dpdt', 'a_epad', 'epad', 's_a_epad', 's_epad', 'min_dpdt', 'max_dpdt',\n",
    "                         'A_p', 'P_max', 'esp', 'sys', 'EF',  'Ees/Ea', 'iT', 'PC1', 'PC2', 'PC3']\n",
    "filtered_output = output[calibrate_output_keys]\n",
    "filtered_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_rows = results_df.loc[calibrate_output_keys]\n",
    "selected_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build beta matrix (d * p, where d is dimension of y_obs and p is dimension of X)\n",
    "beta_matrix = []\n",
    "intercept = []\n",
    "\n",
    "# Selects which observation to calibrate on\n",
    "which_obs = 3\n",
    "\n",
    "for index, row_entry in selected_rows.iterrows():\n",
    "    model = row_entry['Model']\n",
    "    coeffs = model.coef_\n",
    "    b0 = model.intercept_\n",
    "\n",
    "    beta_matrix.append(coeffs)\n",
    "    intercept.append(b0)\n",
    "\n",
    "# Convert the list to a NumPy array\n",
    "beta_matrix = np.array(beta_matrix)\n",
    "intercept = np.array(intercept)\n",
    "intercept = intercept.reshape(len(intercept),1)\n",
    "\n",
    "print(beta_matrix.shape)\n",
    "print(intercept.shape)\n",
    "\n",
    "# Select observation and reshape to be (d, 1)\n",
    "Y_obs = np.array(filtered_output.T[which_obs])\n",
    "Y_obs = Y_obs.reshape(len(Y_obs), 1)\n",
    "print(Y_obs.shape)\n",
    "\n",
    "# Scale observation by intercepts of models\n",
    "Y_scaled = Y_obs - intercept\n",
    "print(Y_scaled.shape)\n",
    "\n",
    "# Compute the pseudo-inverse of the coefficient matrix\n",
    "beta_inv = np.linalg.inv(beta_matrix.T @ beta_matrix) @ beta_matrix.T\n",
    "x_hat = beta_inv @ Y_scaled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the calibrated inputs to a DataFrame\n",
    "x_hat_df = pd.DataFrame(x_hat, index=filtered_input.iloc[which_obs].T.index)\n",
    "\n",
    "# Concatenate the true input value and calibrated values\n",
    "result = pd.concat([filtered_input.iloc[which_obs].T, x_hat_df], axis=1)\n",
    "\n",
    "# Rename the columns\n",
    "result.columns = ['x_true', 'x_calibrated']\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed calibrated x_hat back into linear model \n",
    "y_calibrated = (beta_matrix @ x_hat) + intercept \n",
    "\n",
    "y_compare = np.hstack([Y_obs, y_calibrated])\n",
    "y_compare = pd.DataFrame(y_compare)\n",
    "y_compare.columns = (\"y_true\", \"y_calibrated\")\n",
    "\n",
    "y_compare.to_csv('multiple_output_calibration_result_y.csv', index=False)\n",
    "y_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Observation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ind = which_obs\n",
    "\n",
    "t = range(100) # Time adjustment\n",
    "p_pat = output_file.iloc[ind, :100].values # Pressure transient\n",
    "\n",
    "# Plot the pressure transient for each realization\n",
    "ax.plot(t, p_pat, label=f'Realisation {ind}')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Time Index')\n",
    "ax.set_ylabel('Pressure (mmHg)')\n",
    "ax.set_title('Pressure Transients in Right Ventricle')\n",
    "\n",
    "# Add legend to the plot\n",
    "# ax.legend()\n",
    "fig.set_size_inches(4,2)\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Calibration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define priors\n",
    "# mean\n",
    "c_svn_mu = 20.5\n",
    "r_pat_mu = 0.31\n",
    "c_pat_mu = 3.8\n",
    "rv_mu = 1.15\n",
    "t_mu = 0.61 #1\n",
    "\n",
    "# variances\n",
    "c_svn_sd = 3.42**2\n",
    "r_pat_sd = 0.05**2\n",
    "c_pat_sd = 0.63**2\n",
    "rv_sd = 0.48**2\n",
    "t_sd = 0.0000001 #0.15**2\n",
    "\n",
    "mu_0 = np.array([c_svn_mu, r_pat_mu, c_pat_mu, rv_mu, t_mu])[:, np.newaxis]\n",
    "Sd = [c_svn_sd, r_pat_sd, c_pat_sd, rv_sd, t_sd]\n",
    "Sigma_0 = np.diag(Sd)/5\n",
    "\n",
    "# Build beta matrix (d * p, where d is dimension of y_obs and p is dimension of X)\n",
    "beta_matrix = []\n",
    "intercept = []\n",
    "\n",
    "# Selects which observation to calibrate on\n",
    "which_obs = 3\n",
    "\n",
    "for index, row_entry in selected_rows.iterrows():\n",
    "    model = row_entry['Model']\n",
    "    coeffs = model.coef_\n",
    "    b0 = model.intercept_\n",
    "\n",
    "    beta_matrix.append(coeffs)\n",
    "    intercept.append(b0)\n",
    "\n",
    "# Convert the list to a NumPy array\n",
    "beta_matrix = np.array(beta_matrix)\n",
    "intercept = np.array(intercept)\n",
    "intercept = intercept.reshape(len(intercept),1)\n",
    "\n",
    "# Select observation and reshape to be (d, 1)\n",
    "Y_obs = np.array(filtered_output.T[which_obs])\n",
    "Y_obs = Y_obs.reshape(len(Y_obs), 1)\n",
    "\n",
    "# Scale observation by intercepts of models\n",
    "Y_scaled = Y_obs - intercept\n",
    "\n",
    "# Compute the posterior covariance\n",
    "Sigma_post_inv = (beta_matrix.T @ beta_matrix) + np.linalg.inv(Sigma_0)\n",
    "Sigma_post = np.linalg.inv(Sigma_post_inv)\n",
    "\n",
    "# Cmpute the posterior mean\n",
    "Mu_post = Sigma_post @ (beta_matrix.T @ Y_scaled + np.linalg.inv(Sigma_0) @ mu_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the calibrated inputs to a DataFrame\n",
    "Mu_post_df = pd.DataFrame(Mu_post, index=filtered_input.iloc[which_obs].T.index)\n",
    "true_observation = pd.DataFrame(filtered_input.iloc[which_obs])\n",
    "\n",
    "# Feed calibrated x_hat back into linear model \n",
    "y_calibrated = (beta_matrix @ Mu_post) + intercept \n",
    "\n",
    "y_compare = np.hstack([Y_obs, y_calibrated])\n",
    "y_compare = pd.DataFrame(y_compare)\n",
    "y_compare.columns = (\"y_true\", \"y_calibrated\")\n",
    "\n",
    "# Concatenate the true input value and calibrated values\n",
    "bayes_result = pd.concat([filtered_input.iloc[which_obs].T, Mu_post_df, y_compare], axis=1)\n",
    "\n",
    "# Rename the columns\n",
    "bayes_result.columns = ['x_true', 'x_calibrated', \"y_true\", \"y_calibrated\"]\n",
    "\n",
    "bayes_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Convert (5,1) arrays to (5,) for proper indexing\n",
    "prior_means = mu_0.flatten()\n",
    "prior_stds = np.sqrt(np.diag(Sigma_0))  # Extract standard deviations\n",
    "\n",
    "posterior_means = Mu_post.flatten()\n",
    "posterior_stds = np.sqrt(np.diag(Sigma_post))  # Extract standard deviations\n",
    "\n",
    "# True values from filtered_input\n",
    "true_values = filtered_input.iloc[which_obs].values\n",
    "\n",
    "# Parameter names\n",
    "param_names = [\"c_svn\", \"r_pat\", \"c_pat\", \"rv_Eact\", \"T\"]\n",
    "\n",
    "# Create subplots for distributions\n",
    "fig, axes = plt.subplots(1, 5, figsize=(18, 4))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    # Define x-range based on prior and posterior means\n",
    "    x_min = min(prior_means[i] - 3 * prior_stds[i], posterior_means[i] - 3 * posterior_stds[i])\n",
    "    x_max = max(prior_means[i] + 3 * prior_stds[i], posterior_means[i] + 3 * posterior_stds[i])\n",
    "    x = np.linspace(x_min, x_max, 100)\n",
    "\n",
    "    # Compute PDFs\n",
    "    prior_pdf = norm.pdf(x, prior_means[i], prior_stds[i])\n",
    "    posterior_pdf = norm.pdf(x, posterior_means[i], posterior_stds[i])\n",
    "\n",
    "    # Plot prior and posterior distributions\n",
    "    ax.plot(x, prior_pdf, label=\"Prior\", linestyle=\"dashed\", color=\"blue\")\n",
    "    ax.plot(x, posterior_pdf, label=\"Posterior\", linestyle=\"solid\", color=\"red\")\n",
    "\n",
    "    # Plot true value as a vertical line\n",
    "    ax.axvline(true_values[i], color=\"green\", linestyle=\"dotted\", label=\"True Value\")\n",
    "\n",
    "    # Labels and title\n",
    "    ax.set_title(param_names[i])\n",
    "    ax.set_xlabel(\"Value\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- Plot Covariance Matrices ----\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot Prior Covariance Matrix\n",
    "sns.heatmap(Sigma_0, annot=True, fmt=\".3f\", cmap=\"RdBu\", xticklabels=param_names, yticklabels=param_names, ax=axes[0])\n",
    "axes[0].set_title(\"Prior Covariance Matrix\")\n",
    "\n",
    "# Plot Posterior Covariance Matrix\n",
    "sns.heatmap(Sigma_post, annot=True, fmt=\".4f\", cmap=\"PiYG\", xticklabels=param_names, yticklabels=param_names, ax=axes[1])\n",
    "axes[1].set_title(\"Posterior Covariance Matrix\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE of Bayes Calibration for Multiple Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define priors\n",
    "# mean\n",
    "c_svn_mu = 20.5\n",
    "r_pat_mu = 0.31\n",
    "c_pat_mu = 3.8\n",
    "rv_mu = 1.15\n",
    "t_mu = 1\n",
    "\n",
    "# variances\n",
    "c_svn_sd = 3.42**2\n",
    "r_pat_sd = 0.05**2\n",
    "c_pat_sd = 0.63**2\n",
    "rv_sd = 0.48**2\n",
    "t_sd = 0.15**2\n",
    "\n",
    "mu_0 = np.array([c_svn_mu, r_pat_mu, c_pat_mu, rv_mu, t_mu])[:, np.newaxis]\n",
    "Sd = [c_svn_sd, r_pat_sd, c_pat_sd, rv_sd, t_sd]\n",
    "Sigma_0 = np.diag(Sd)\n",
    "\n",
    "# Build beta matrix (d * p, where d is dimension of y_obs and p is dinemnsion of X)\n",
    "beta_matrix = []\n",
    "intercept = []\n",
    "\n",
    "for index, row_entry in selected_rows.iterrows():\n",
    "    model = row_entry['Model']\n",
    "    coeffs = model.coef_\n",
    "    b0 = model.intercept_\n",
    "\n",
    "    beta_matrix.append(coeffs)\n",
    "    intercept.append(b0)\n",
    "\n",
    "# Convert the list to a NumPy array\n",
    "beta_matrix = np.array(beta_matrix)\n",
    "intercept = np.array(intercept)\n",
    "intercept = intercept.reshape(len(intercept),1)\n",
    "\n",
    "x_differences = []\n",
    "\n",
    "for row in range(10):\n",
    " # Select observation and reshape to be (d, 1)\n",
    " Y_obs = np.array(filtered_output.T[row])\n",
    " Y_obs = Y_obs.reshape(len(Y_obs), 1)\n",
    " \n",
    " # Scale observation by intercepts of models\n",
    " Y_scaled = Y_obs - intercept\n",
    "\n",
    "\n",
    " # Compute the posterior covariance\n",
    " Sigma_post_inv = (beta_matrix.T @ beta_matrix) + np.linalg.inv(Sigma_0)\n",
    " Sigma_post = np.linalg.inv(Sigma_post_inv)\n",
    "\n",
    " # Cmpute the posterior mean\n",
    " Mu_post = Sigma_post @ (beta_matrix.T @ Y_scaled + np.linalg.inv(Sigma_0) @ mu_0)\n",
    "\n",
    " # Compute squared-diff between true and calibrated x\n",
    " true = np.array(filtered_input.iloc[row].T)\n",
    " true = true.reshape(len(mu_0),1)\n",
    " diff = (Mu_post - true)**2 \n",
    "\n",
    " # Append arrary\n",
    " x_differences.append(diff)\n",
    "\n",
    "# Compute MSE\n",
    "bayes_mse_x = np.mean(np.hstack(x_differences), axis=1)\n",
    "\n",
    "bayes_mse_x_df = pd.DataFrame(bayes_mse_x)\n",
    "bayes_mse_x_df.columns = ['MSE']\n",
    "bayes_mse_x_df.index = filtered_input.columns\n",
    "#bayes_mse_x_df.to_csv('bayes_MSE_multi_output_x.csv')\n",
    "\n",
    "bayes_mse_x_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including model error and observation error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Calibration  \n",
    "Assume we have the following linear regression model $$y = \\beta X + \\beta_0 + \\epsilon_{model}, \\epsilon_{model} \\sim N_p(0, \\sigma_{model}^2I)$$ \n",
    "\n",
    "Assume we also have the following observation model $$y_{obs} = y + \\epsilon_{obs}, \\epsilon_{obs} \\sim N_p(0, \\sigma_{obs}^2I)$$. \n",
    "\n",
    "The posterior distribution can be shown that $$\\pi(X|y_{obs}) \\sim N(\\mu_{post}, \\Sigma_{post})$$ where $$\\mu_{post} = \\Sigma_{post} \\left( \\beta^T \\left( \\sigma_{model}^2 + \\sigma_{obs}^2\\right)^{-1} y_{obs} + \\Sigma_0^{-1}\\mu_0\\right)$$ and $$ \\Sigma_{post} = \\left( \\beta^T( \\sigma_{model}^2 + \\sigma_{obs}^2)^{-1}\\beta + \\Sigma_0^{-1}\\right)^{-1}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects which observation to calibrate on\n",
    "which_obs = 1\n",
    "\n",
    "## Define priors\n",
    "# mean\n",
    "c_svn_mu = 20.5\n",
    "r_pat_mu = 0.31\n",
    "c_pat_mu = 3.8\n",
    "rv_mu = 1.15\n",
    "t_mu = filtered_input['T'][which_obs]\n",
    "\n",
    "# variances\n",
    "c_svn_sd = 3.42**2\n",
    "r_pat_sd = 0.05**2\n",
    "c_pat_sd = 0.63**2\n",
    "rv_sd = 0.48**2\n",
    "t_sd = 0.00000001 #0.15**2\n",
    "\n",
    "mu_0 = np.array([c_svn_mu, r_pat_mu, c_pat_mu, rv_mu, t_mu])[:, np.newaxis]\n",
    "Sd = [c_svn_sd, r_pat_sd, c_pat_sd, rv_sd, t_sd]\n",
    "Sigma_0 = np.diag(Sd)\n",
    "\n",
    "\n",
    "# Model error\n",
    "epsilon_model = np.diag(selected_rows['RSE']**2)\n",
    "\n",
    "# Observation error \n",
    "obs_error_scale = 0.05\n",
    "obs_error = np.std(filtered_output)*obs_error_scale\n",
    "epsilon_obs = np.diag(obs_error)\n",
    "\n",
    "# total error\n",
    "full_error = epsilon_obs + epsilon_model\n",
    "\n",
    "# Build beta matrix (d * p, where d is dimension of y_obs and p is dimension of X)\n",
    "beta_matrix = []\n",
    "intercept = []\n",
    "\n",
    "\n",
    "for index, row_entry in selected_rows.iterrows():\n",
    "    model = row_entry['Model']\n",
    "    coeffs = model.coef_\n",
    "    b0 = model.intercept_\n",
    "\n",
    "    beta_matrix.append(coeffs)\n",
    "    intercept.append(b0)\n",
    "\n",
    "# Convert the list to a NumPy array\n",
    "beta_matrix = np.array(beta_matrix)\n",
    "intercept = np.array(intercept)\n",
    "intercept = intercept.reshape(len(intercept),1)\n",
    "\n",
    "# Select observation and reshape to be (d, 1)\n",
    "Y_obs = np.array(filtered_output.T[which_obs])\n",
    "Y_obs = Y_obs.reshape(len(Y_obs), 1)\n",
    "\n",
    "# Scale observation by intercepts of models\n",
    "Y_scaled = Y_obs - intercept\n",
    "\n",
    "# Compute the posterior covariance\n",
    "Sigma_post_inv = (beta_matrix.T @ np.linalg.inv(full_error) @ beta_matrix) + np.linalg.inv(Sigma_0)\n",
    "Sigma_post = np.linalg.inv(Sigma_post_inv)\n",
    "\n",
    "# Cmpute the posterior mean\n",
    "Mu_post = Sigma_post @ (beta_matrix.T @ np.linalg.inv(full_error) @ Y_scaled + np.linalg.inv(Sigma_0) @ mu_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Convert (5,1) arrays to (5,) for proper indexing\n",
    "prior_means = mu_0.flatten()\n",
    "prior_stds = np.sqrt(np.diag(Sigma_0))  # Extract standard deviations\n",
    "\n",
    "posterior_means = Mu_post.flatten()\n",
    "posterior_stds = np.sqrt(np.diag(Sigma_post))  # Extract standard deviations\n",
    "\n",
    "# True values from filtered_input\n",
    "true_values = filtered_input.iloc[which_obs].values\n",
    "\n",
    "# Parameter names\n",
    "param_names = [\"c_svn\", \"r_pat\", \"c_pat\", \"rv_Eact\", \"T\"]\n",
    "\n",
    "# Create subplots for distributions\n",
    "fig, axes = plt.subplots(1, 5, figsize=(18, 4))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    # Define x-range based on prior and posterior means\n",
    "    x_min = min(prior_means[i] - 3 * prior_stds[i], posterior_means[i] - 3 * posterior_stds[i])\n",
    "    x_max = max(prior_means[i] + 3 * prior_stds[i], posterior_means[i] + 3 * posterior_stds[i])\n",
    "    x = np.linspace(x_min, x_max, 100)\n",
    "\n",
    "    # Compute PDFs\n",
    "    prior_pdf = norm.pdf(x, prior_means[i], prior_stds[i])\n",
    "    posterior_pdf = norm.pdf(x, posterior_means[i], posterior_stds[i])\n",
    "\n",
    "    # Plot prior and posterior distributions\n",
    "    ax.plot(x, prior_pdf, label=\"Prior\", linestyle=\"dashed\", color=\"blue\")\n",
    "    ax.plot(x, posterior_pdf, label=\"Posterior\", linestyle=\"solid\", color=\"red\")\n",
    "\n",
    "    # Plot true value as a vertical line\n",
    "    ax.axvline(true_values[i], color=\"green\", linestyle=\"dotted\", label=\"True Value\")\n",
    "\n",
    "    # Labels and title\n",
    "    ax.set_title(param_names[i])\n",
    "    ax.set_xlabel(\"Value\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- Plot Covariance Matrices ----\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot Prior Covariance Matrix\n",
    "sns.heatmap(Sigma_0, annot=True, fmt=\".3f\", cmap=\"RdBu\", xticklabels=param_names, yticklabels=param_names, ax=axes[0])\n",
    "axes[0].set_title(\"Prior Covariance Matrix\")\n",
    "\n",
    "# Plot Posterior Covariance Matrix\n",
    "sns.heatmap(Sigma_post, annot=True, fmt=\".4f\", cmap=\"PiYG\", xticklabels=param_names, yticklabels=param_names, ax=axes[1])\n",
    "axes[1].set_title(\"Posterior Covariance Matrix\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AnalysisGiessen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
